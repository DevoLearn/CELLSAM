{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q monai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone -b batch_size https://github.com/sushmanthreddy/segment-anything.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd segment-anything/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls -a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -e .","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport monai\nfrom os import makedirs\nfrom os.path import join\nfrom tqdm import tqdm\nfrom time import time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom datetime import datetime\nfrom segment_anything import sam_model_registry\nimport cv2\nfrom matplotlib import pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ..","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%ls -a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(path):\n  dirs = os.listdir( path )\n  for item in tqdm(dirs):\n    if os.path.isfile(path+item):\n      im = Image.open(path+item)\n      f, e = os.path.splitext(path+item)\n      imResize = im.resize((1024,1024), Image.NEAREST)\n      imResize.save(f+e, 'PNG', quality=100)\n\nlabel_path =  \"/kaggle/input/nucleus-data/nucleus_data/segmentation_maps\"\noutput_features_path = \"/kaggle/input/nucleus-data/nucleus_data/features\"\nresize(label_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import listdir\nfrom os.path import isfile, join\nimport pandas as pd\n\n\nids=[]\nlabel_filenames = [f for f in listdir(label_path) if isfile(join(label_path, f))]\nfeature_filenames = [f for f in listdir(output_features_path) if isfile(join(output_features_path, f))]\nfor i in range(len(feature_filenames)):\n  ids.append(feature_filenames[i][1:])\nprint(len(ids))\n\ndf = pd.DataFrame(ids ,columns=[\"file_ids\"])\ndf.to_csv('full_file_ids.csv', index=False)\n\n#sanity check\ndf = pd.read_csv('full_file_ids.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport cv2\n\ndf = pd.read_csv('full_file_ids.csv')\nids = df['file_ids'].tolist()\nnon_empty_ids = []\n\nfor file_id in ids:\n    mask_path = os.path.join(label_path, 'L' + file_id)\n    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    if cv2.countNonZero(mask) > 0:\n        non_empty_ids.append(file_id)\n\ndf_non_empty = pd.DataFrame(non_empty_ids, columns=[\"file_ids\"])\ndf_non_empty.sort_values(by='file_ids', inplace=True)  # Sort the DataFrame by 'file_ids'\ndf_non_empty.to_csv('file_ids.csv', index=False)\n\n\ndif = pd.read_csv('file_ids.csv')\ndif.head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n%mkdir checkpoint_save","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_root = \"/kaggle/input/nucleus-data/nucleus_data\"\nwork_dir = \"/kaggle/working/checkpoint_save\"\nweight_decay = 0.01\nlr = 0.0001\nnum_epochs = 10\nbatch_size = 4\nnum_workers = 8\ncheckpoint = \"/kaggle/working/sam_vit_b_01ec64.pth\"\ndata_aug = True\nseed = 2023\ndevice = \"cuda:0\"\nresume = \"\"\n\n\ntorch.cuda.empty_cache()\nos.environ['PYTHONHASHSEED']=str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(work_dir, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SegmentationDataset(Dataset):\n    def __init__(self, csv_file, data_aug=True):\n        self.df = pd.read_csv(csv_file)\n        self.ids = self.df[\"file_ids\"]\n        self.img_path = \"/kaggle/input/nucleus-data/nucleus_data/features\"\n        self.mask_path = \"/kaggle/input/nucleus-data/nucleus_data/segmentation_maps\"\n        self.data_aug = data_aug\n        print(f\"number of images: {len(self.ids)}\")\n        \n    def __len__(self):\n        return len(self.ids)\n    \n    def __getitem__(self, index):\n        img_name = f\"F{self.ids[index]}\"\n        mask_name = f\"L{self.ids[index]}\"\n        \n        # Load image\n        img = Image.open(join(self.img_path, img_name)).resize((1024, 1024)).convert(\"RGB\")\n        img = np.array(img)\n        img = np.transpose(img, (2, 0, 1))\n        img = img / 255.0\n        \n        # Load mask\n        mask = Image.open(join(self.mask_path, mask_name)).resize((1024, 1024))\n        mask = np.array(mask)\n        \n        label_ids = np.unique(mask)[1:]\n        try:\n            gt2D = np.uint8(mask == random.choice(label_ids.tolist())) # only one label, (256, 256)\n        except:\n            print(img_name, 'label_ids.tolist()', label_ids.tolist())\n            gt2D = np.uint8(mask == np.max(mask)) # only one label, (256, 256)\n        \n        # Data augmentation\n        if self.data_aug:\n            if random.random() > 0.5:\n                img = np.ascontiguousarray(np.flip(img, axis=-1))\n                gt2D = np.ascontiguousarray(np.flip(gt2D, axis=-1))\n            if random.random() > 0.5:\n                img = np.ascontiguousarray(np.flip(img, axis=-2))\n                gt2D = np.ascontiguousarray(np.flip(gt2D, axis=-2))\n        \n        # Coords Calculation\n        gt2D = np.uint8(gt2D > 0)\n        y_indices, x_indices = np.where(gt2D > 0)\n        x_point = np.random.choice(x_indices)\n        y_point = np.random.choice(y_indices)\n        coords = np.array([x_point, y_point])\n\n        # Resize mask to 256x256\n        gt2D_256 = cv2.resize(gt2D, (256, 256), interpolation=cv2.INTER_NEAREST)\n        \n        return {\n            \"image\": torch.tensor(img).float(),\n            \"gt2D\": torch.tensor(gt2D_256[None, :,:]).long(),\n            \"coords\": torch.tensor(coords[None, ...]).float(),\n            \"image_name\": img_name\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CellSAM(nn.Module):\n    def __init__(self, \n                image_encoder, \n                mask_decoder,\n                prompt_encoder,\n                ):\n        super().__init__()\n        self.image_encoder = image_encoder\n        self.mask_decoder = mask_decoder\n        self.prompt_encoder = prompt_encoder\n\n        # freeze prompt encoder\n        for param in self.prompt_encoder.parameters():\n            param.requires_grad = False\n        \n        \n        for param in self.image_encoder.parameters():\n            param.requires_grad = False\n\n    def forward(self, image, point_prompt):\n\n        # do not compute gradients for pretrained img encoder and prompt encoder\n        with torch.no_grad():\n            image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n            # not need to convert box to 1024x1024 grid\n            # bbox is already in 1024x1024\n            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n                points=point_prompt,\n                boxes=None,\n                masks=None,\n            )\n        low_res_masks, iou_predictions = self.mask_decoder(\n            image_embeddings=image_embedding, # (B, 256, 64, 64)\n            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n            multimask_output=False,\n          ) # (B, 1, 256, 256)\n\n        return low_res_masks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sam_model = sam_model_registry[\"vit_b\"](checkpoint=checkpoint)\ncellsam_model = CellSAM(\n    image_encoder = sam_model.image_encoder,\n    mask_decoder = sam_model.mask_decoder,\n    prompt_encoder = sam_model.prompt_encoder,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cellsam_model = cellsam_model.to(device)\ncellsam_model.train()\nprint(f\"CellSAM size: {sum(p.numel() for p in cellsam_model.parameters())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.AdamW(\n    cellsam_model.mask_decoder.parameters(),\n    lr=lr,\n    betas=(0.9, 0.999),\n    eps=1e-08,\n    weight_decay=weight_decay\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_aug = True\n\nseg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction='mean')\nce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\ntrain_dataset = SegmentationDataset(csv_file=\"file_ids.csv\", data_aug=data_aug)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resume = resume\nif resume:\n    checkpoint = torch.load(resume)\n    cellsam_model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    start_epoch = checkpoint[\"epoch\"] + 1\n    best_loss = checkpoint[\"best_loss\"]\n    print(f\"Loaded checkpoint from epoch {start_epoch}, best loss: {best_loss:.4f}\")\nelse:\n    start_epoch = 0\n    best_loss = 1e10\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_time = []\nlosses = []\nfor epoch in range(start_epoch, num_epochs):\n    epoch_loss = [1e10 for _ in range(len(train_loader))]\n    epoch_start_time = time()\n    pbar = tqdm(train_loader)\n    for step, batch in enumerate(pbar):\n        image = batch[\"image\"]\n        gt2D = batch[\"gt2D\"]\n        coords_torch = batch[\"coords\"] # (B, 2)\n        optimizer.zero_grad()\n        labels_torch = torch.ones(coords_torch.shape[0]).long() # (B,)\n        labels_torch = labels_torch.unsqueeze(1) # (B, 1)\n        image, gt2D = image.to(device), gt2D.to(device)\n        coords_torch, labels_torch = coords_torch.to(device), labels_torch.to(device)\n        point_prompt = (coords_torch, labels_torch)\n        cellsam_lite_pred = cellsam_model(image, point_prompt)\n        loss = seg_loss(cellsam_lite_pred, gt2D) + ce_loss(cellsam_lite_pred, gt2D.float())\n        epoch_loss[step] = loss.item()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        pbar.set_description(f\"Epoch {epoch} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, loss: {loss.item():.4f}\")\n\n    epoch_end_time = time()\n    epoch_time.append(epoch_end_time - epoch_start_time)\n    epoch_loss_reduced = sum(epoch_loss) / len(epoch_loss)\n    losses.append(epoch_loss_reduced)\n    model_weights = cellsam_model.state_dict()\n    checkpoint = {\n        \"model\": model_weights,\n        \"epoch\": epoch,\n        \"optimizer\": optimizer.state_dict(),\n        \"loss\": epoch_loss_reduced,\n        \"best_loss\": best_loss\n    }\n    if epoch_loss_reduced < best_loss:\n        print(f\"New best loss: {best_loss:.4f} -> {epoch_loss_reduced:.4f}\")\n        best_loss = epoch_loss_reduced\n        checkpoint[\"best_loss\"] = best_loss\n        torch.save(checkpoint, join(work_dir, \"cellsam_point_prompt_best.pth\"))\n\n    torch.save(checkpoint, join(work_dir, \"cellsam_point_prompt_latest.pth\"))\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n    ax1.plot(losses)\n    ax1.set_title(\"Dice + Cross Entropy Loss\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax2.plot(epoch_time)\n    ax2.set_title(\"Epoch Running Time\")\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"Time (s)\")\n    fig.savefig(join(work_dir, \"cellsam_point_prompt_loss_time.png\"))\n\n    epoch_loss_reduced = 1e10\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}